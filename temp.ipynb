{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "\n",
    "    def __init__(self, input_file):\n",
    "        with open(input_file, 'r') as f:\n",
    "            data = f.read()\n",
    "        words = data.splitlines()\n",
    "        words = [w.strip() for w in words] # get rid of any leading or trailing white space\n",
    "        words = [w for w in words if w] # get rid of any empty strings\n",
    "        chars = sorted(list(set(''.join(words)))) # all the possible characters\n",
    "        max_word_length = max(len(w) for w in words)\n",
    "        print(f\"number of examples in the dataset: {len(words)}\")\n",
    "        print(f\"max word length: {max_word_length}\")\n",
    "        print(f\"number of unique characters in the vocabulary: {len(chars)}\")\n",
    "        print(\"vocabulary:\")\n",
    "        print(''.join(chars))\n",
    "        self.words = words\n",
    "        self.chars = chars\n",
    "        print(\"chars: {chars}\")\n",
    "        self.stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "        self.itos = {i:s for s,i in self.stoi.items()} \n",
    "        # self.generate_tokens(input_file)\n",
    "        \n",
    "    def generate_tokens(self,input_file):\n",
    "        tokens=[]\n",
    "        for item in self.words:\n",
    "            tokens.extend([self.stoi[c] for c in item]) \n",
    "        # merged_tokens = torch.cat(all_tokens, dim=0)\n",
    "        tokens_np = np.array(tokens)\n",
    "        assert (0 <= tokens_np).all() and (tokens_np < 2**16).all(), \"token dictionary too large for uint16\"\n",
    "        tokens_np_uint16 = tokens_np.astype(np.uint16)\n",
    "        file_name = input_file + '_tokens'\n",
    "        np.save(file_name, tokens_np_uint16)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "\n",
    "    def contains(self, word):\n",
    "        return word in self.words\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.chars) + 1 # all the possible characters and special 0 token\n",
    "\n",
    "    def encode(self, word):\n",
    "        ix = torch.tensor([self.stoi[w] for w in word], dtype=torch.long)\n",
    "        return ix\n",
    "\n",
    "    def decode(self, ix):\n",
    "        word = ''.join(self.itos[i] for i in ix)\n",
    "        return word\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        word = self.words[idx]\n",
    "        ix = self.encode(word)\n",
    "        tkns= torch.tensor(ix, dtype=torch.long)#x, y\n",
    "        return tkns\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokens(filename):\n",
    "    npt = np.load(filename)\n",
    "    npt = npt.astype(np.int32) # added after video\n",
    "    ptt = torch.tensor(npt, dtype=torch.long)\n",
    "    return ptt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4, 38, 34,  ..., 43, 26, 39])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6750857"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# # wrap in dataset objects\n",
    "# file_name = 'babynames_train_0'\n",
    "# CharDataset('./babynames/babynames_0.txt')\n",
    "#token_filename = file_name + '_tokens.npy'\n",
    "ptt = load_tokens('./babynames/babynames_train_0.txt_tokens.npy')\n",
    "len(ptt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206.01980590820312"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6750857/32768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Read the Parquet file\n",
    "df = pd.read_parquet('./raw_data/names_val.parquet')\n",
    "# Select the specific column you want\n",
    "column_data = df['Names']  # Replace 'your_column_name' with the actual column name\n",
    "# Write the selected column to a text file\n",
    "column_data.to_csv('babynames_val.txt', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "with open('./babynames/babynames_val.txt', 'r') as f:\n",
    "        data = f.read()\n",
    "words = data.splitlines()\n",
    "words = [w.strip() for w in words] # get rid of any leading or trailing white space\n",
    "words = [w for w in words if w]\n",
    "tokens = []\n",
    "chars = sorted(list(set(''.join(words)))) \n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "for item in words:\n",
    "        tokens.extend([stoi[c] for c in item]) \n",
    "# merged_tokens = torch.cat(all_tokens, dim=0)\n",
    "tokens_np = np.array(tokens)\n",
    "assert (0 <= tokens_np).all() and (tokens_np < 2**16).all(), \"token dictionary too large for uint16\"\n",
    "tokens_np_uint16 = tokens_np.astype(np.uint16)\n",
    "file_name = 'babynames_val_' + '_tokens'\n",
    "np.save(file_name, tokens_np_uint16)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.11.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
