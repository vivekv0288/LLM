{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "\n",
    "    def __init__(self, input_file):\n",
    "        with open(input_file, 'r') as f:\n",
    "            data = f.read()\n",
    "        words = data.splitlines()\n",
    "        words = [w.strip() for w in words] # get rid of any leading or trailing white space\n",
    "        words = [w for w in words if w] # get rid of any empty strings\n",
    "        chars = sorted(list(set(''.join(words)))) # all the possible characters\n",
    "        chars.append('.')\n",
    "        max_word_length = max(len(w) for w in words)\n",
    "        print(f\"number of examples in the dataset: {len(words)}\")\n",
    "        print(f\"max word length: {max_word_length}\")\n",
    "        print(f\"number of unique characters in the vocabulary: {len(chars)}\")\n",
    "        print(\"vocabulary:\")\n",
    "        print(''.join(chars))\n",
    "        self.words = words\n",
    "        self.chars = chars\n",
    "        print(\"chars: {chars}\")\n",
    "        self.stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "        self.itos = {i:s for s,i in self.stoi.items()} \n",
    "        self.generate_tokens(input_file,max_word_length)\n",
    "     \n",
    "\n",
    "    def add_period_if_short(self,item):\n",
    "        for i in range(17):\n",
    "            if len(item) < i:\n",
    "                item += '.'\n",
    "        return item\n",
    "\n",
    "   \n",
    "    def generate_tokens(self,input_file,max_word_length):\n",
    "        tokens=[]\n",
    "        for item in self.words:\n",
    "            wrd = self.add_period_if_short(item)\n",
    "            tokens.extend([self.stoi[c] for c in wrd]) \n",
    "        # merged_tokens = torch.cat(all_tokens, dim=0)\n",
    "        tokens_np = np.array(tokens)\n",
    "        assert (0 <= tokens_np).all() and (tokens_np < 2**16).all(), \"token dictionary too large for uint16\"\n",
    "        tokens_np_uint16 = tokens_np.astype(np.uint16)\n",
    "        file_name = input_file + '_tokens'\n",
    "        np.save(file_name, tokens_np_uint16)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "\n",
    "    def contains(self, word):\n",
    "        return word in self.words\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.chars) + 1 # all the possible characters and special 0 token\n",
    "\n",
    "    def encode(self, word):\n",
    "        ix = torch.tensor([self.stoi[w] for w in word], dtype=torch.long)\n",
    "        return ix\n",
    "\n",
    "    def decode(self, ix):\n",
    "        word = ''.join(self.itos[i] for i in ix)\n",
    "        return word\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        word = self.words[idx]\n",
    "        ix = self.encode(word)\n",
    "        tkns= torch.tensor(ix, dtype=torch.long)#x, y\n",
    "        return tkns\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokens(filename):\n",
    "    npt = np.load(filename)\n",
    "    npt = npt.astype(np.int32) # added after video\n",
    "    ptt = torch.tensor(npt, dtype=torch.long)\n",
    "    return ptt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab='ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz.'\n",
    "vocab =''.join(sorted(vocab))\n",
    "vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples in the dataset: 271663\n",
      "max word length: 15\n",
      "number of unique characters in the vocabulary: 53\n",
      "vocabulary:\n",
      "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz.\n",
      "chars: {chars}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.CharDataset at 0x1187d2410>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# # wrap in dataset objects\n",
    "# file_name = 'babynames_train_0'\n",
    "# CharDataset('./babynames/babynames.txt')\n",
    "#token_filename = file_name + '_tokens.npy'\n",
    "# ptt = load_tokens('./babynames/babynames_train_0.txt_tokens.npy')\n",
    "# len(ptt)\n",
    "CharDataset('./babynames/babynames_val.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6750857/32768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Read the Parquet file\n",
    "df = pd.read_parquet('./raw_data/names_val.parquet')\n",
    "# Select the specific column you want\n",
    "column_data = df['Names']  # Replace 'your_column_name' with the actual column name\n",
    "# Write the selected column to a text file\n",
    "column_data.to_csv('babynames_val.txt', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "with open('./babynames/babynames_val.txt', 'r') as f:\n",
    "        data = f.read()\n",
    "words = data.splitlines()\n",
    "words = [w.strip() for w in words] # get rid of any leading or trailing white space\n",
    "words = [w for w in words if w]\n",
    "tokens = []\n",
    "chars = sorted(list(set(''.join(words)))) \n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "for item in words:\n",
    "        tokens.extend([stoi[c] for c in item]) \n",
    "# merged_tokens = torch.cat(all_tokens, dim=0)\n",
    "tokens_np = np.array(tokens)\n",
    "assert (0 <= tokens_np).all() and (tokens_np < 2**16).all(), \"token dictionary too large for uint16\"\n",
    "tokens_np_uint16 = tokens_np.astype(np.uint16)\n",
    "file_name = 'babynames_val_' + '_tokens'\n",
    "np.save(file_name, tokens_np_uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_until_next_uppercase(s):\n",
    "    result = []\n",
    "    temp = ''\n",
    "\n",
    "    for char in s:\n",
    "        if char.isupper() and temp:\n",
    "            result.append(temp)\n",
    "            temp = ''\n",
    "        temp += char\n",
    "    \n",
    "    if temp:  # Append the last accumulated string if any\n",
    "        result.append(temp)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "input_string = \"HelloWorldPythonIsAwesome\"\n",
    "segments = fetch_until_next_uppercase(input_string)\n",
    "def add_period_if_short(s):\n",
    "    for i in range(15):\n",
    "        if len(s) < i:\n",
    "            s += '.'\n",
    "    return s\n",
    "\n",
    "print(\"Segments:\", segments)\n",
    "for s in segments:\n",
    "    print(add_period_if_short(s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Example usage\n",
    "file_path = 'babynames/babynames.txt' \n",
    "input_string = \n",
    "result = add_period_if_short(input_string)\n",
    "print(\"Result:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_names(file_path):\n",
    "    unique_names = set()\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            name = line.strip()  # Remove leading/trailing whitespace, including newline characters\n",
    "            if name:  # Ensure the line is not empty\n",
    "                unique_names.add(name)\n",
    "    \n",
    "    return len(unique_names)\n",
    "\n",
    "# Example usage\n",
    "file_path = 'babynames/babynames.txt'  # Replace with the path to your file\n",
    "unique_count = count_unique_names(file_path)\n",
    "print(f\"Number of unique names: {unique_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "npt = np.load('babynames/data_tokens/babynames_train_tokens.npy')\n",
    "npt = npt.astype(np.int32) # added after video\n",
    "ptt = torch.tensor(npt, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(16):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ram............', 15)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_period_if_short1(item):\n",
    "    for i in range(16):\n",
    "        if len(item) < i:\n",
    "            item += '.'\n",
    "    \n",
    "    return item\n",
    "\n",
    "name  =add_period_if_short1('ram')\n",
    "name, len(name)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.11.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
