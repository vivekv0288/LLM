{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "\n",
    "    def __init__(self, input_file):\n",
    "        with open(input_file, 'r') as f:\n",
    "            data = f.read()\n",
    "        words = data.splitlines()\n",
    "        words = [w.strip() for w in words] # get rid of any leading or trailing white space\n",
    "        words = [w for w in words if w] # get rid of any empty strings\n",
    "        chars = sorted(list(set(''.join(words)))) # all the possible characters\n",
    "        chars.append('.')\n",
    "        max_word_length = max(len(w) for w in words)\n",
    "        print(f\"number of examples in the dataset: {len(words)}\")\n",
    "        print(f\"max word length: {max_word_length}\")\n",
    "        print(f\"number of unique characters in the vocabulary: {len(chars)}\")\n",
    "        print(\"vocabulary:\")\n",
    "        print(''.join(chars))\n",
    "        self.words = words\n",
    "        self.chars = chars\n",
    "        print(\"chars: {chars}\")\n",
    "        self.stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "        self.itos = {i:s for s,i in self.stoi.items()} \n",
    "        #self.generate_tokens(input_file,max_word_length)\n",
    "        self.createFixedLengthDataSet(input_file)\n",
    "     \n",
    "\n",
    "    def add_period_if_short(self,item):\n",
    "        for i in range(17):\n",
    "            if len(item) < i:\n",
    "                item += '.'\n",
    "        return item\n",
    "\n",
    "   \n",
    "    def generate_tokens(self,input_file,max_word_length):\n",
    "        tokens=[]\n",
    "        for item in self.words:\n",
    "            wrd = self.add_period_if_short(item)\n",
    "            tokens.extend([self.stoi[c] for c in wrd]) \n",
    "        # merged_tokens = torch.cat(all_tokens, dim=0)\n",
    "        tokens_np = np.array(tokens)\n",
    "        assert (0 <= tokens_np).all() and (tokens_np < 2**16).all(), \"token dictionary too large for uint16\"\n",
    "        tokens_np_uint16 = tokens_np.astype(np.uint16)\n",
    "        file_name = input_file + '_tokens'\n",
    "        np.save(file_name, tokens_np_uint16)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "\n",
    "    def contains(self, word):\n",
    "        return word in self.words\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.chars) + 1 # all the possible characters and special 0 token\n",
    "\n",
    "    def encode(self, word):\n",
    "        ix = torch.tensor([self.stoi[w] for w in word], dtype=torch.long)\n",
    "        return ix\n",
    "\n",
    "    def decode(self, ix):\n",
    "        word = ''.join(self.itos[i] for i in ix)\n",
    "        return word\n",
    "    \n",
    "    def createFixedLengthDataSet(self,input_file):\n",
    "        with open(input_file, 'r') as f:\n",
    "            data = f.read()\n",
    "        words = data.splitlines()\n",
    "        words = [w.strip() for w in words] # get rid of any leading or trailing white space\n",
    "        words = [w for w in words if w]\n",
    "        result = \".\"+ \".\".join(words)\n",
    "        # Break the result into chunks of length 64\n",
    "        # result_chunks = [result[i:i+64] for i in range(0, len(result), 64)]\n",
    "        batch_number = 0\n",
    "        index = 0\n",
    "        batches = []\n",
    "        while True:\n",
    "            isAvailaible, data, index = self.fetchNextFromDot(result, index)\n",
    "            if not isAvailaible or len(data) < 64:\n",
    "                break\n",
    "            batches.append(data)\n",
    "        \n",
    "        tokens = []\n",
    "        for batch in batches:\n",
    "            tokens.extend([self.stoi[c] for c in batch]) \n",
    "            \n",
    "        tokens_np = np.array(tokens)\n",
    "        assert (0 <= tokens_np).all() and (tokens_np < 2**16).all(), \"token dictionary too large for uint16\"\n",
    "        tokens_np_uint16 = tokens_np.astype(np.uint16)\n",
    "        file_name = input_file + 'train_tokens'\n",
    "        np.save(file_name, tokens_np_uint16)\n",
    "    \n",
    "    def fetchNextFromDot(self, result, index):\n",
    "        batch_size = 64\n",
    "        if index == 0:\n",
    "            return  True, result[:batch_size], batch_size\n",
    "        \n",
    "        data = result[index: index+batch_size]\n",
    "        \n",
    "        index_of_first_dot = data.find('.')\n",
    "        if index_of_first_dot == -1:\n",
    "            return False, None, None\n",
    "        \n",
    "        data = data[index_of_first_dot:]\n",
    "        remaining = result[index+batch_size: index+batch_size+index_of_first_dot]\n",
    "        data += remaining\n",
    "        \n",
    "        return True, data, index+batch_size+index_of_first_dot\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        word = self.words[idx]\n",
    "        ix = self.encode(word)\n",
    "        tkns= torch.tensor(ix, dtype=torch.long)#x, y\n",
    "        return tkns\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # wrap in dataset objects\n",
    "# file_name = 'babynames_train_0'\n",
    "# CharDataset('./babynames/babynames.txt')\n",
    "#token_filename = file_name + '_tokens.npy'\n",
    "# ptt = load_tokens('./babynames/babynames_train_0.txt_tokens.npy')\n",
    "# len(ptt)\n",
    "CharDataset('./babynames/babynames_val.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6750857/32768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Read the Parquet file\n",
    "df = pd.read_parquet('./raw_data/names_val.parquet')\n",
    "# Select the specific column you want\n",
    "column_data = df['Names']  # Replace 'your_column_name' with the actual column name\n",
    "# Write the selected column to a text file\n",
    "column_data.to_csv('babynames_val.txt', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "with open('./babynames/babynames_val.txt', 'r') as f:\n",
    "        data = f.read()\n",
    "words = data.splitlines()\n",
    "words = [w.strip() for w in words] # get rid of any leading or trailing white space\n",
    "words = [w for w in words if w]\n",
    "tokens = []\n",
    "chars = sorted(list(set(''.join(words)))) \n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "for item in words:\n",
    "        tokens.extend([stoi[c] for c in item]) \n",
    "# merged_tokens = torch.cat(all_tokens, dim=0)\n",
    "tokens_np = np.array(tokens)\n",
    "assert (0 <= tokens_np).all() and (tokens_np < 2**16).all(), \"token dictionary too large for uint16\"\n",
    "tokens_np_uint16 = tokens_np.astype(np.uint16)\n",
    "file_name = 'babynames_val_' + '_tokens'\n",
    "np.save(file_name, tokens_np_uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_until_next_uppercase(s):\n",
    "    result = []\n",
    "    temp = ''\n",
    "\n",
    "    for char in s:\n",
    "        if char.isupper() and temp:\n",
    "            result.append(temp)\n",
    "            temp = ''\n",
    "        temp += char\n",
    "    \n",
    "    if temp:  # Append the last accumulated string if any\n",
    "        result.append(temp)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "input_string = \"HelloWorldPythonIsAwesome\"\n",
    "segments = fetch_until_next_uppercase(input_string)\n",
    "def add_period_if_short(s):\n",
    "    for i in range(15):\n",
    "        if len(s) < i:\n",
    "            s += '.'\n",
    "    return s\n",
    "\n",
    "print(\"Segments:\", segments)\n",
    "for s in segments:\n",
    "    print(add_period_if_short(s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Example usage\n",
    "file_path = 'babynames/babynames.txt' \n",
    "input_string = \n",
    "result = add_period_if_short(input_string)\n",
    "print(\"Result:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_names(file_path):\n",
    "    unique_names = set()\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            name = line.strip()  # Remove leading/trailing whitespace, including newline characters\n",
    "            if name:  # Ensure the line is not empty\n",
    "                unique_names.add(name)\n",
    "    \n",
    "    return len(unique_names)\n",
    "\n",
    "# Example usage\n",
    "file_path = 'babynames/babynames.txt'  # Replace with the path to your file\n",
    "unique_count = count_unique_names(file_path)\n",
    "print(f\"Number of unique names: {unique_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "npt = np.load('babynames/data_tokens/babynames_train_tokens.npy')\n",
    "npt = npt.astype(np.int32) # added after video\n",
    "ptt = torch.tensor(npt, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(16):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_period_if_short1(item):\n",
    "    for i in range(16):\n",
    "        if len(item) < i:\n",
    "            item += '.'\n",
    "    \n",
    "    return item\n",
    "\n",
    "name  =add_period_if_short1('ram')\n",
    "name, len(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "checkpoint = torch.load(\"./log/model.pt\", mmap=False)\n",
    "# Initialize the model using the configuration saved in the checkpoint\n",
    "model = GPT(checkpoint['config'])\n",
    "\n",
    "# Load the saved state dict into the model\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model.to(device)\n",
    "use_compile = False\n",
    "# Optionally, retrieve the training step to continue from where it left off\n",
    "trained_step = checkpoint['step']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"jbrazzy/baby_names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = 32\n",
    "layers = 12\n",
    "\n",
    "total_parametrs = emb*emb*3*layers\n",
    "total_parametrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "256*16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1024*64*1\n",
    "T*B*N = .5M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "with open('./babynames/babynames_val.txt', 'r') as f:\n",
    "        data = f.read()\n",
    "words = data.splitlines()\n",
    "words = [w.strip() for w in words] # get rid of any leading or trailing white space\n",
    "words = [w for w in words if w]\n",
    "tokens = []\n",
    "chars = sorted(list(set(''.join(words)))) \n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "for item in words:\n",
    "        tokens.extend([stoi[c] for c in item]) \n",
    "# merged_tokens = torch.cat(all_tokens, dim=0)\n",
    "tokens_np = np.array(tokens)\n",
    "assert (0 <= tokens_np).all() and (tokens_np < 2**16).all(), \"token dictionary too large for uint16\"\n",
    "tokens_np_uint16 = tokens_np.astype(np.uint16)\n",
    "file_name = 'babynames_val_' + '_tokens'\n",
    "np.save(file_name, tokens_np_uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "with open('./babynames/babynames.txt', 'r') as f:\n",
    "        data = f.read()\n",
    "words = data.splitlines()\n",
    "words = [w.strip() for w in words] # get rid of any leading or trailing white space\n",
    "words = [w for w in words if w]\n",
    "result = \".\"+ \".\".join(words)\n",
    "# Break the result into chunks of length 64\n",
    "# result_chunks = [result[i:i+64] for i in range(0, len(result), 64)]\n",
    "batch_number = 0\n",
    "index = 0\n",
    "batches = []\n",
    "while True:\n",
    "    isAvailaible, data, index = fetchNextFromDot(result, index)\n",
    "    if not isAvailaible or len(data) < 64:\n",
    "        break\n",
    "    batches.append(data)\n",
    "    \n",
    "def fetchNextFromDot(result, index):\n",
    "    batch_size = 64\n",
    "    if index == 0:\n",
    "        return  True, result[:batch_size], batch_size\n",
    "    \n",
    "    data = result[index: index+batch_size]\n",
    "    \n",
    "    index_of_first_dot = data.find('.')\n",
    "    if index_of_first_dot == -1:\n",
    "        return False, None, None\n",
    "    \n",
    "    data = data[index_of_first_dot:]\n",
    "    remaining = result[index+batch_size: index+batch_size+index_of_first_dot]\n",
    "    data += remaining\n",
    "    \n",
    "    return True, data, index+batch_size+index_of_first_dot\n",
    "    \n",
    "\n",
    "# Print first few chunks to verify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(batches)*64)/65536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetchNextFromDot(result, index):\n",
    "    batch_size = 64\n",
    "    if index == 0:\n",
    "        return  True, result[:batch_size], batch_size\n",
    "    \n",
    "    data = result[index: index+batch_size]\n",
    "    \n",
    "    index_of_first_dot = data.find('.')\n",
    "    if index_of_first_dot == -1:\n",
    "        return False, None, None\n",
    "    \n",
    "    data = data[index_of_first_dot:]\n",
    "    remaining = result[index+batch_size: index+batch_size+index_of_first_dot]\n",
    "    data += remaining\n",
    "    \n",
    "    return True, data, index+batch_size+index_of_first_dot\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_chunks[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"jbrazzy/baby_names\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = dataset['train']['Names']\n",
    "# words = data.splitlines()\n",
    "words = [w.strip() for w in words] # get rid of any leading or trailing white space\n",
    "words = [w for w in words if w] # get rid of any empty strings\n",
    "chars = sorted(list(set(''.join(words)))) # all the possible characters\n",
    "chars.append('.')\n",
    "max_word_length = max(len(w) for w in words)\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:s for s,i in stoi.items()} \n",
    "\n",
    "\n",
    "def fetchNextFromDot( result, index):\n",
    "    batch_size = 64\n",
    "    if index == 0:\n",
    "        return  True, result[:batch_size], batch_size\n",
    "    \n",
    "    data = result[index: index+batch_size]\n",
    "    \n",
    "    index_of_first_dot = data.find('.')\n",
    "    if index_of_first_dot == -1:\n",
    "        return False, None, None\n",
    "    \n",
    "    data = data[index_of_first_dot:]\n",
    "    remaining = result[index+batch_size: index+batch_size+index_of_first_dot]\n",
    "    data += remaining\n",
    "    \n",
    "    return True, data, index+batch_size+index_of_first_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "batches = []\n",
    "result = \".\"+ \".\".join(words)\n",
    "while True:\n",
    "    isAvailaible, data, index = fetchNextFromDot(result, index)\n",
    "    if not isAvailaible or len(data) < 64:\n",
    "        break\n",
    "    batches.append(data)\n",
    "\n",
    "tokens = []\n",
    "for batch in batches:\n",
    "    tokens.extend([stoi[c] for c in batch]) \n",
    "    \n",
    "tokens_np = np.array(tokens)\n",
    "assert (0 <= tokens_np).all() and (tokens_np < 2**16).all(), \"token dictionary too large for uint16\"\n",
    "tokens_np_uint16 = tokens_np.astype(np.uint16)\n",
    "np.save(config.train_token_file, tokens_np_uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7454848, array([52,  4, 38, 34, 37, 50, 52,  7, 26, 39], dtype=uint16))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens_np_uint16), tokens_np_uint16[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the 'Names' column from the dataset\n",
    "data = dataset['train']['Names']\n",
    "\n",
    "# Apply the replace function to each name in the list\n",
    "cleaned_data = [name.replace(',', '').replace(\"'\", '') for name in data]\n",
    "\n",
    "# Print the first few cleaned names\n",
    "print(cleaned_data[:10])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.11.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
