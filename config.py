vocab='ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz.'
vocab_size: int = 53 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token
n_embd: int = 64 # embedding dimension
n_layer: int = 4 # number of layers
n_head: int = 4 # number of heads
total_batch_size = 65536 # 2**19, ~0.5M, in number of tokens
batch_size = 256 # micro batch size
block_size = 64 # sequence length
max_lr = 6e-4
warmup_steps = 50
training_steps = 10
generate_steps = 5 # once in a while generate from the model 
num_return_sequences = 4
generate_max_length = 64
# block_size: int = 64 # max sequence length
root_path = "drive/MyDrive/ColabData"
data_root = root_path + "/babynames"
tokens_dir = "tokens"
train_token_file = tokens_dir+"/train_tokens.npy"
log_dir = root_path+ "/log"
checkpoint_path = log_dir+'/model.pt'
val_file_name = data_root+"/babynames_val.txt"
train_file_name = data_root+"/babynames_train.txt"