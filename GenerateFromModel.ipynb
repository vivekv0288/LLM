{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1ZjV8we5xyIuoEiqr32HSj4cT4RjcJJWB","authorship_tag":"ABX9TyPeZZxlPz9whN6p9lySBGVo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"dAP5WUdRRvlP","executionInfo":{"status":"ok","timestamp":1726313684303,"user_tz":240,"elapsed":6631,"user":{"displayName":"Vishwajeet singh","userId":"04805818380920126572"}}},"outputs":[],"source":["import os\n","import math\n","import time\n","import inspect\n","from dataclasses import dataclass\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","import random\n","\n","class CausalSelfAttention(nn.Module):\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        assert config.n_embd % config.n_head == 0\n","        # key, query, value projections for all heads, but in a batch\n","        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n","        # output projection\n","        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n","        self.c_proj.NANOGPT_SCALE_INIT = 1\n","        # regularization\n","        self.n_head = config.n_head\n","        self.n_embd = config.n_embd\n","\n","    def forward(self, x):\n","        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n","        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n","        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n","        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n","        qkv = self.c_attn(x)\n","        q, k, v = qkv.split(self.n_embd, dim=2)\n","        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n","        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n","        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n","        y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # flash attention\n","        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n","        # output projection\n","        y = self.c_proj(y)\n","        return y\n","\n","class MLP(nn.Module):\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n","        self.gelu    = nn.GELU(approximate='tanh')\n","        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n","        self.c_proj.NANOGPT_SCALE_INIT = 1\n","\n","    def forward(self, x):\n","        x = self.c_fc(x)\n","        x = self.gelu(x)\n","        x = self.c_proj(x)\n","        return x\n","\n","class Block(nn.Module):\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.ln_1 = nn.LayerNorm(config.n_embd)\n","        self.attn = CausalSelfAttention(config)\n","        self.ln_2 = nn.LayerNorm(config.n_embd)\n","        self.mlp = MLP(config)\n","\n","    def forward(self, x):\n","        x = x + self.attn(self.ln_1(x))\n","        x = x + self.mlp(self.ln_2(x))\n","        return x\n","\n","@dataclass\n","class GPTConfig:\n","    block_size: int = 64 # max sequence length\n","    vocab_size: int = 53 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n","    n_layer: int = 4 # number of layers\n","    n_head: int = 4 # number of heads\n","    n_embd: int = 64 # embedding dimension\n","\n","class GPT(nn.Module):\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","\n","        self.transformer = nn.ModuleDict(dict(\n","            wte = nn.Embedding(config.vocab_size, config.n_embd),\n","            wpe = nn.Embedding(config.block_size, config.n_embd),\n","            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n","            ln_f = nn.LayerNorm(config.n_embd),\n","        ))\n","        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n","\n","        # weight sharing scheme\n","        self.transformer.wte.weight = self.lm_head.weight\n","\n","        # init params\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            std = 0.02\n","            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n","                std *= (2 * self.config.n_layer) ** -0.5\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n","            if module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","        elif isinstance(module, nn.Embedding):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","\n","    def forward(self, idx, targets=None):\n","        # idx is of shape (B, T)\n","        #1, 256\n","        B, T = idx.size()\n","        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n","        # forward the token and posisition embeddings\n","        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n","        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n","        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n","        #1,256,768\n","\n","        x = tok_emb + pos_emb\n","        # forward the blocks of the transformer\n","        for block in self.transformer.h:\n","            x = block(x)\n","        # forward the final layernorm and the classifier\n","        x = self.transformer.ln_f(x)\n","        logits = self.lm_head(x) # (B, T, vocab_size)\n","        loss = None\n","        if targets is not None:\n","            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n","        return logits, loss\n","\n","    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n","        # start with all of the candidate parameters (that require grad)\n","        param_dict = {pn: p for pn, p in self.named_parameters()}\n","        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n","        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n","        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n","        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n","        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n","        optim_groups = [\n","            {'params': decay_params, 'weight_decay': weight_decay},\n","            {'params': nodecay_params, 'weight_decay': 0.0}\n","        ]\n","        num_decay_params = sum(p.numel() for p in decay_params)\n","        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n","        if master_process:\n","            print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n","            print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n","        # Create AdamW optimizer and use the fused version if it is available\n","        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n","        use_fused = fused_available and device_type == \"cuda\"\n","        if master_process:\n","            print(f\"using fused AdamW: {use_fused}\")\n","        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n","        return optimizer"]},{"cell_type":"code","source":["def encode( word):\n","        vocab='ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz.'\n","        stoi = {ch:i for i,ch in enumerate(vocab)}\n","        itos = {i:s for s,i in stoi.items()}\n","        ix = torch.tensor([stoi[w] for w in word], dtype=torch.long)\n","        return ix\n","def decode( ix):\n","        vocab='ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz.'\n","        stoi = {ch:i for i,ch in enumerate(vocab)}\n","        itos = {i:s for s,i in stoi.items()}\n","        word = ''.join(itos[i] for i in ix)\n","        return word"],"metadata":{"id":"ZrwfYkGuSaI9","executionInfo":{"status":"ok","timestamp":1726313686970,"user_tz":240,"elapsed":197,"user":{"displayName":"Vishwajeet singh","userId":"04805818380920126572"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["ddp_rank = 0\n","ddp_local_rank = 0\n","ddp_world_size = 1\n","master_process = True\n","# attempt to autodetect device\n","device = \"cpu\"\n","if torch.cuda.is_available():\n","    device = \"cuda\"\n","elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n","    device = \"mps\"\n","print(f\"using device: {device}\")\n","\n","# added after video, pytorch can be serious about it's device vs. device_type distinction\n","device_type = \"cuda\" if device.startswith(\"cuda\") else \"cpu\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OTHw9bgVTyxG","executionInfo":{"status":"ok","timestamp":1726313714288,"user_tz":240,"elapsed":201,"user":{"displayName":"Vishwajeet singh","userId":"04805818380920126572"}},"outputId":"4c0ec81b-cfd2-4564-b798-fdea45f5f899"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["using device: cuda\n"]}]},{"cell_type":"code","source":["pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"TBcQe6VMUqKj","executionInfo":{"status":"ok","timestamp":1726313837125,"user_tz":240,"elapsed":211,"user":{"displayName":"Vishwajeet singh","userId":"04805818380920126572"}},"outputId":"1fbaed59-c0b7-41d3-fcb9-22f08efc48ae"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["checkpoint = torch.load('drive/MyDrive/ColabData/log/model.pt')\n","\n","# Initialize the model using the configuration saved in the checkpoint\n","model = GPT(checkpoint['config'])\n","\n","# Load the saved state dict into the model\n","model.load_state_dict(checkpoint['model'])\n","model.to(device)\n","use_compile = False\n","# Optionally, retrieve the training step to continue from where it left off\n","trained_step = checkpoint['step']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f8CM_ooFUWrr","executionInfo":{"status":"ok","timestamp":1726313877068,"user_tz":240,"elapsed":1304,"user":{"displayName":"Vishwajeet singh","userId":"04805818380920126572"}},"outputId":"7c308272-9b4d-4e60-9047-d70a24799ecb"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-6-8b9d0ea1dddb>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load('drive/MyDrive/ColabData/log/model.pt')\n"]}]},{"cell_type":"code","source":["model.eval()\n","num_return_sequences = 4\n","max_length = 64\n","random_number = random.randint(0, 25)\n","tokens =[52, random_number]\n","tokens = torch.tensor(tokens, dtype=torch.long)\n","tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n","xgen = tokens.to(device)\n","sample_rng = torch.Generator(device=device)\n","sample_rng.manual_seed(42 + ddp_rank)\n","while xgen.size(1) < max_length:\n","    # forward the model to get the logits\n","    with torch.no_grad():\n","        with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n","            logits, loss = model(xgen) # (B, T, vocab_size)\n","        # take the logits at the last position\n","        logits = logits[:, -1, :] # (B, vocab_size)\n","        # get the probabilities\n","        probs = F.softmax(logits, dim=-1)\n","        # do top-k sampling of 50 (huggingface pipeline default)\n","        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n","        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n","        # select a token from the top-k probabilities\n","        # note: multinomial does not demand the input to sum to 1\n","        ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n","        # gather the corresponding indices\n","        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n","        # append to the sequence\n","        xgen = torch.cat((xgen, xcol), dim=1)\n","# print the generated text\n","for i in range(num_return_sequences):\n","    tokens = xgen[i, :max_length].tolist()\n","    decoded = decode(tokens)\n","    print(f\"rank {ddp_rank} sample {i}: {decoded}\")"],"metadata":{"id":"n27sZPOoU_A6","executionInfo":{"status":"ok","timestamp":1726313942358,"user_tz":240,"elapsed":1855,"user":{"displayName":"Vishwajeet singh","userId":"04805818380920126572"}},"outputId":"a279db24-d274-4fb8-a6f6-42255ac594d6","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["rank 0 sample 0: .Aleth.Ayyan.Alamy.Ally.Aysha.Azbis.Am.Amydadr.Agylle.Aleyn.Anny\n","rank 0 sample 1: .Aron.Aryn.Cajestona.Danton.Evolrin.Edan.Ellemoth.Eteron.Fray.Fa\n","rank 0 sample 2: .Antay.Angech.Tyer.Taniell.Tasha.Tamarri.Hanricai.Jarshe.Laya.Vi\n","rank 0 sample 3: .Abarlenn.AHahdbe.Haarchef.Kara.Talal.Brelons.Koyd.Matlay.Nila.P\n"]}]}]}